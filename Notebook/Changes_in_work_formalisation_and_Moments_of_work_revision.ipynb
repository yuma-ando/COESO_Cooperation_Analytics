{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z381_WTYLDes"
   },
   "source": [
    "Changes in work formalisation and Moments of work revision sub-indicators are calculated monthly based on email data. The goal is to detect/extract organizational and revision patterns in samples in email communication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApnbQR062aJ3"
   },
   "source": [
    "Please use Tool_for_extracting_filtering_and_cleaning_GMAIL_data.ipynb for extracting data from mbox file and saving it to csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yeuIJ5JlfRit",
    "outputId": "1d370761-c238-4f73-b2a5-74f0431a9def"
   },
   "outputs": [],
   "source": [
    "#install and imprort needed libraries\n",
    "!pip install --upgrade gensim\n",
    "!pip install --upgrade language_tool_python\n",
    "!pip install googletrans==4.0.0-rc1\n",
    "from re import sub\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity\n",
    "# Load the model: this is a big file, can take a while to download and open\n",
    "glove = api.load(\"glove-wiki-gigaword-50\")\n",
    "similarity_index = WordEmbeddingSimilarityIndex(glove)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "!pip install langdetect\n",
    "from langdetect import detect\n",
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFO9TNo9fY1y"
   },
   "source": [
    "These sub-indicators require special approach to the preprocessing, because we need to clean emails text from noise that may impact the quality of semantic search that we use for organizational and revision patterns detection. This part was done manually so it's need to be automitised. The issue here is the fact that we are working with many languages (English, French, Italian etc).\n",
    "\n",
    "Preprocessing steps:\n",
    "\n",
    "1. remove signatures: \n",
    "2. remove \"best regards synonyms\". À très bientôt en chair et en os,\n",
    "3. remove URls/emails and \">\" symbols\n",
    "4. proper names but not inside the sentences, only at the begging and at the end of email, including abbreviations at the end of message like T/DB\n",
    "5. replay comments also should be removed till the end, keeping only first original message: Le ven. 18 mars 2022 à 08:26, XXXXX < XXXXX@XXXXX> a écrit/ On Mon, May 2, 2022 at 8:47 AM XXXXX@XXXXX wrote:/ ---------- Forwarded message ---------\n",
    "\n",
    "6. remove greetings: Dear XXXX, dear XXXX, Bonjour XXXX, Dear COESO WP4  and WP2  Colleagues,Dear All\n",
    "replies info inside letter: From: XXXXXX XXXXX@XXXXX Date: Tuesday, 8 March 2022 at 17:33 To: XXXX XXXXX@XXXX.es Subject: Pregunta para proyecto COESO Citizen Science\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-2PvCA9sUu8"
   },
   "outputs": [],
   "source": [
    "# reading .csv cleaned dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"put_csv_data_here.csv\")\n",
    "\n",
    "# previewing the first 5 lines of the loaded data\n",
    "data.head()\n",
    "\n",
    "#Translate text to English, we do it to have all communication in English\n",
    "\n",
    "data['Translated'] = data['Body'].apply(lambda x: translator.translate(x, dest='en').text)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZbR3A4YfXL1",
    "outputId": "5f6c77eb-1a93-4f69-8dcc-5146e96df154"
   },
   "outputs": [],
   "source": [
    "# this step is needed to sort data month by month, but in real life data will come month by month and you will process it at the end of a given month.\n",
    "month_sort = data.loc[data.Date.str.contains('Jun', na=False)]    #Sep, Oct, Nov, Dec, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug\n",
    "text = month_sort[\"Translated\"].to_list()\n",
    "print(\"original values: \")\n",
    "print(len(text))\n",
    "\n",
    "def remove_duplicates(sents):\n",
    "  unique_sents = []\n",
    "  for sent in sents:\n",
    "    if sent not in unique_sents:\n",
    "      unique_sents.append(sent)\n",
    "  return unique_sents\n",
    "\n",
    "subjects = remove_duplicates(text)\n",
    "print(\"without duplicates: \")\n",
    "print(len(subjects))\n",
    "\n",
    "\n",
    "#additional cleaning to make sure that we removed all noisy text\n",
    "import re\n",
    "import string\n",
    "from six.moves.html_parser import HTMLParser\n",
    "h = HTMLParser()\n",
    "target = string.printable + \"öäüÖÄÜ\"\n",
    "\n",
    "def clean(value):\n",
    "  sentences = []\n",
    "  for v in subjects:\n",
    "    if type(v) is not str:\n",
    "        continue\n",
    "    doc = re.sub (\"\\n\", \" \", v)\n",
    "    doc = re.sub (\"\\r\", \" \",doc)\n",
    "    doc = re.sub(\"http\\S+\", \" \", doc) #removing urls\n",
    "    doc = re.sub(\"[`~!@#$%^&*()_|+\\-=?;:'<>\\{\\}\\[\\]\\\\\\/]\", ' ', doc)\n",
    "    doc = h.unescape(doc) #converting other HTML entities to recongisable characters\n",
    "    doc = re.sub(\"(\\s|\\t){2,}\", \" \", doc) #removing unnecessary spaces\n",
    "    doc = re.sub(\"\\S*@\\S*\\s?\", \" \", doc) #removing emails\n",
    "    doc = re.sub(\"@[^\\d]\", \" \", doc) #removing phone numbers\n",
    "    sentences.append(doc.strip()) #removing leading and trailing spaces and adding clean string to the list\n",
    "  return sentences\n",
    "\n",
    "\n",
    "clean_sents = clean(subjects)\n",
    "print(\"clean_sents\")\n",
    "print(len(clean_sents))\n",
    "\n",
    "\n",
    "###tokenising text into sentences\n",
    "full_text = '.'.join(str(x) for x in clean_sents)\n",
    "splitted_sentences = nltk.sent_tokenize(full_text)\n",
    "\n",
    "print(\"splitted_sentences\")\n",
    "print(len(splitted_sentences))\n",
    "\n",
    "###creating new table\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.set_option('display.max_columns', 0)\n",
    "documents_df=pd.DataFrame(splitted_sentences,columns=['splitted_sentences'])\n",
    "df_new = documents_df[documents_df['splitted_sentences'].notnull()]\n",
    "\n",
    "\n",
    "text_messag = df_new['splitted_sentences'].to_list()\n",
    "print(\"only eng\")\n",
    "print(len(text_messag))\n",
    "\n",
    "\n",
    "###removing duplicates\n",
    "new_subjects = remove_duplicates(text_messag)\n",
    "print(\"second round check without duplicates:\")\n",
    "print(len(new_subjects))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk3GpnJClBLb"
   },
   "source": [
    "SIMILARITY CHECK (Soft Cosine Measure):\n",
    "we use semantic check to see how close semantically are samples out of emails and our target vocabularies of organizational and revision work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24Zyq1EflCCV"
   },
   "outputs": [],
   "source": [
    "###dictionary with contextual phrases###\n",
    "d = dict()\n",
    "d['organizational'] = [\"Work organisation thus refers to how work is planned, organised and managed within companies and to choices on a range of aspects such as work processes, job design, responsibilities, task allocation, work scheduling, work pace, rules and procedures, and decision-making processes.\"]\n",
    "d['revision'] = [\"revision\", \"revise\", \"redo\", \"revising\", \"re-\", \"revised\", \"redone\", \"redraft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvo8cmrClrj1"
   },
   "outputs": [],
   "source": [
    "samples = {}\n",
    "for (name, v) in d.items():\n",
    "    query_string = str(v)\n",
    "\n",
    "    documents = new_subjects\n",
    "\n",
    "\n",
    "    def preprocess(doc):\n",
    "        # Tokenize, clean up input document string\n",
    "        doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
    "        doc = sub(r'\\n', '', doc)\n",
    "        doc = sub(r'http\\S+', '', doc)\n",
    "        doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
    "        doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
    "        doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
    "        return [token for token in simple_preprocess(doc, deacc=False, min_len=2, max_len=15)]\n",
    "\n",
    "    # Preprocess the documents, including the query string\n",
    "    corpus = [preprocess(document) for document in documents]\n",
    "    query = preprocess(query_string)\n",
    "    raw_corpus = len(corpus)\n",
    "    print(raw_corpus)\n",
    "\n",
    "    # Build the term dictionary, TF-idf model\n",
    "    dictionary = Dictionary(corpus+[query])\n",
    "    tfidf = TfidfModel(dictionary=dictionary)\n",
    "\n",
    "    # Create the term similarity matrix.\n",
    "    similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)\n",
    "\n",
    "\n",
    "    # Compute Soft Cosine Measure between the query and the documents.\n",
    "    query_tf = tfidf[dictionary.doc2bow(query)]\n",
    "\n",
    "    index = SoftCosineSimilarity(\n",
    "                tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
    "                similarity_matrix)\n",
    "\n",
    "    doc_similarity_scores = index[query_tf]\n",
    "\n",
    "    # Output the sorted similarity scores and documents\n",
    "    sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
    "    samples[name] = 0\n",
    "    print(name)\n",
    "    for idx in sorted_indexes:\n",
    "        if doc_similarity_scores[idx] > 0.3:\n",
    "            samples[name] = samples[name] + 1\n",
    "print(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohHB2fVx9jFg",
    "outputId": "6ef0eb58-eb90-4e17-cfdd-5726f90ceea3"
   },
   "outputs": [],
   "source": [
    "print(samples) #count of samples where the model detected implicit and explicit markers of organizaional and revision work\n",
    "len_org = samples.get(\"organizational\")\n",
    "len_rev = samples.get(\"revision\")\n",
    "print(len_org)\n",
    "print(len_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77tNRyffqIsv",
    "outputId": "e39dc745-d927-4fb7-a3fd-8691c97e2e6d"
   },
   "outputs": [],
   "source": [
    "#calculate the percentage of samples with implicit and explicit markers of organizational and revision work out of the total count of samples and present them as fraction of 1\n",
    "def is_what_percent_of(num_a, num_b):\n",
    "    return (num_a / num_b) * 100\n",
    "\n",
    "org_percent = is_what_percent_of (len_org, raw_corpus)\n",
    "organization = org_percent / 100\n",
    "print(\"Changes in work formalisation is {:0.2f}.\".format(organization))\n",
    "\n",
    "rev_percent = is_what_percent_of (len_rev, raw_corpus)\n",
    "revision = rev_percent /100\n",
    "print(\"Moments of work revision is {:0.2f}.\".format(revision))\n",
    "\n",
    "#No additional normalization is needed, as we are already in the scale from 0 to 1\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
